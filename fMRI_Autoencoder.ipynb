{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOXhaoC+tjmDsKxby90apZ8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ichiyan/BrainDecoding/blob/master/fMRI_Autoencoder.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vXJQ76Y6FBp9"
      },
      "outputs": [],
      "source": [
        "!pip install tensorflow-gpu\n",
        "!pip install bdpy"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Installation not using GPU:  !pip install tensorflow as tf"
      ],
      "metadata": {
        "id": "eYQKCu5VFh-r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import torch\n",
        "import torchvision\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from torchvision import transforms\n",
        "from torchvision.datasets import MNIST\n",
        "from torch.autograd import Variable\n",
        "from matplotlib import pyplot as plt\n",
        "import numpy as np\n",
        "import bdpy\n",
        "from bdpy import BData\n",
        "print(tf.__version__)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6vSStnPWFyi4",
        "outputId": "b381d578-1d38-4e2c-ab69-e5e8c664847f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.9.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3xcBb3H9OQP0",
        "outputId": "76008387-642e-427d-fb2c-d32cae780912"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Read data"
      ],
      "metadata": {
        "id": "IzRVK-EonOow"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dat = bdpy.BData('/content/drive/MyDrive/THESIS-Brainstorming/Data/sub-01_perceptionNaturalImageTraining_original_VC.h5')\n",
        "sample = dat.select('ROI_VC = 1')\n",
        "print(sample)\n",
        "print(len(sample[0]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M6CQeFZxIwFe",
        "outputId": "a9d55a86-d3e9-455e-c0ed-bc4be97a0502"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[-7.95109247  4.67269165 -0.36804574 ...  2.45975614  9.20861941\n",
            " -6.8299304 ]\n",
            "11726\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load BData from 'data_file.h5'\n",
        "bdata.load('/content/drive/MyDrive/THESIS-Brainstorming/Data/sub-01_perceptionNaturalImageTraining_original_VC.h5')"
      ],
      "metadata": {
        "id": "egKaRYvcLz0d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Show 'key' and 'description' of metadata\n",
        "# bdata.show_meatadata()\n",
        "bdata.show_metadata\n",
        "\n",
        "# Get 'value' of the metadata specified by 'key'\n",
        "voxel_x = bdata.get_metadata('voxel_x', where='VoxelData')\n",
        "print(voxel_x)"
      ],
      "metadata": {
        "id": "QhWiEUHsL1kU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "192a336e-c7a4-4f09-bff6-80ae8ad41d55"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[-67. -67. -67. ...  57.  57.  57.]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get an array of voxel data in V1\n",
        "data_v1 = bdata.select('ROI_V1')  # shape=(M, num voxels in V1)\n",
        "\n",
        "# `select` accepts some operators\n",
        "data_v1v2 = bdata.select('ROI_V1 + ROI_V2')\n",
        "#data_hvc = bdata.select('ROI_LOC + ROI_FFA + ROI_PPA - LOC_LVC')\n",
        "\n",
        "# Wildcard\n",
        "data_visual = bdata.select('ROI_V*')\n",
        "\n",
        "# Get labels ('image_index') in the dataset\n",
        "label_a  = bdata.select('image_index')"
      ],
      "metadata": {
        "id": "_CDdhWdem-s6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = BData('/content/drive/MyDrive/THESIS-Brainstorming/Data/sub-01_perceptionNaturalImageTraining_original_VC.h5')\n",
        "test_data = BData('/content/drive/MyDrive/THESIS-Brainstorming/Data/sub-01_perceptionNaturalImageTest_original_VC.h5')"
      ],
      "metadata": {
        "id": "999t06MVFCFp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = DataLoader(dataset=ds_train, batch_size=batch_size, shuffle=True, num_workers=1, pin_memory=cuda)\n",
        "test_loader = DataLoader(dataset=ds_test, batch_size=batch_size, shuffle=True, num_workers=1, pin_memory=cuda)"
      ],
      "metadata": {
        "id": "QDaf8zsYoD_x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "NUM_PARCELS = 352\n",
        "SPLIT = 0.3\n",
        "\n",
        "class DenseTied(Layer):\n",
        "    def __init__(self, units,\n",
        "                 activation=None,\n",
        "                 use_bias=True,\n",
        "                 kernel_initializer='glorot_uniform',\n",
        "                 bias_initializer='zeros',\n",
        "                 kernel_regularizer=None,\n",
        "                 bias_regularizer=None,\n",
        "                 activity_regularizer=None,\n",
        "                 kernel_constraint=None,\n",
        "                 bias_constraint=None,\n",
        "                 tied_to=None,\n",
        "                 **kwargs):\n",
        "        self.tied_to = tied_to\n",
        "        if 'input_shape' not in kwargs and 'input_dim' in kwargs:\n",
        "            kwargs['input_shape'] = (kwargs.pop('input_dim'),)\n",
        "        super().__init__(**kwargs)\n",
        "        self.units = units\n",
        "        self.activation = activations.get(activation)\n",
        "        self.use_bias = use_bias\n",
        "        self.kernel_initializer = initializers.get(kernel_initializer)\n",
        "        self.bias_initializer = initializers.get(bias_initializer)\n",
        "        self.kernel_regularizer = regularizers.get(kernel_regularizer)\n",
        "        self.bias_regularizer = regularizers.get(bias_regularizer)\n",
        "        self.activity_regularizer = regularizers.get(activity_regularizer)\n",
        "        self.kernel_constraint = constraints.get(kernel_constraint)\n",
        "        self.bias_constraint = constraints.get(bias_constraint)\n",
        "        self.input_spec = InputSpec(min_ndim=2)\n",
        "        self.supports_masking = True\n",
        "                \n",
        "    def build(self, input_shape):\n",
        "        assert len(input_shape) >= 2\n",
        "        input_dim = input_shape[-1]\n",
        "\n",
        "        if self.tied_to is not None:\n",
        "            self.kernel = K.transpose(self.tied_to.kernel)\n",
        "            self._non_trainable_weights.append(self.kernel)\n",
        "        else:\n",
        "            self.kernel = self.add_weight(shape=(input_dim, self.units),\n",
        "                                          initializer=self.kernel_initializer,\n",
        "                                          name='kernel',\n",
        "                                          regularizer=self.kernel_regularizer,\n",
        "                                          constraint=self.kernel_constraint)\n",
        "        if self.use_bias:\n",
        "            self.bias = self.add_weight(shape=(self.units,),\n",
        "                                        initializer=self.bias_initializer,\n",
        "                                        name='bias',\n",
        "                                        regularizer=self.bias_regularizer,\n",
        "                                        constraint=self.bias_constraint)\n",
        "        else:\n",
        "            self.bias = None\n",
        "        self.input_spec = InputSpec(min_ndim=2, axes={-1: input_dim})\n",
        "        self.built = True\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        assert input_shape and len(input_shape) >= 2\n",
        "        output_shape = list(input_shape)\n",
        "        output_shape[-1] = self.units\n",
        "        return tuple(output_shape)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        output = K.dot(inputs, self.kernel)\n",
        "        if self.use_bias:\n",
        "            output = K.bias_add(output, self.bias, data_format='channels_last')\n",
        "        if self.activation is not None:\n",
        "            output = self.activation(output)\n",
        "        return output\n",
        "\n",
        "class WeightsOrthogonalityConstraint(Constraint):\n",
        "    def __init__(self, encoding_dim, weightage = 1.0, axis = 0):\n",
        "        self.encoding_dim = encoding_dim\n",
        "        self.weightage = weightage\n",
        "        self.axis = axis\n",
        "        \n",
        "    def weights_orthogonality(self, w):\n",
        "        if(self.axis==1):\n",
        "            w = K.transpose(w)\n",
        "        if(self.encoding_dim > 1):\n",
        "            m = K.dot(K.transpose(w), w) - K.eye(self.encoding_dim)\n",
        "            return self.weightage * K.sqrt(K.sum(K.square(m)))\n",
        "        else:\n",
        "            m = K.sum(w ** 2) - 1.\n",
        "            return m\n",
        "\n",
        "    def __call__(self, w):\n",
        "        return self.weights_orthogonality(w)\n",
        "\n",
        "class UncorrelatedFeaturesConstraint (Constraint):\n",
        "    \n",
        "    def __init__(self, encoding_dim, weightage = 1.0):\n",
        "        self.encoding_dim = encoding_dim\n",
        "        self.weightage = weightage\n",
        "    \n",
        "    def get_covariance(self, x):\n",
        "        x_centered_list = []\n",
        "\n",
        "        for i in range(self.encoding_dim):\n",
        "            x_centered_list.append(x[:, i] - K.mean(x[:, i]))\n",
        "        \n",
        "        x_centered = tf.stack(x_centered_list)\n",
        "        covariance = K.dot(x_centered, K.transpose(x_centered)) / tf.cast(x_centered.get_shape()[0], tf.float32)\n",
        "        \n",
        "        return covariance\n",
        "            \n",
        "    # Constraint penalty\n",
        "    def uncorrelated_feature(self, x):\n",
        "        if(self.encoding_dim <= 1):\n",
        "            return 0.0\n",
        "        else:\n",
        "            output = K.sum(K.square(\n",
        "                self.covariance - tf.multiply(self.covariance, K.eye(self.encoding_dim))))\n",
        "            return output\n",
        "\n",
        "    def __call__(self, x):\n",
        "        self.covariance = self.get_covariance(x)\n",
        "        return self.weightage * self.uncorrelated_feature(x)\n",
        "\n",
        "def plot_autoencoder_outputs(autoencoder, test_data):\n",
        "    decoded_pconns = autoencoder.predict(test_data)\n",
        "\n",
        "    # number of example pconns to show\n",
        "    n = 3\n",
        "    fig = plt.figure(figsize=(8,6))\n",
        "    for i in range(n):\n",
        "        # plot original pconn\n",
        "        ax = plt.subplot(2, n, i+1)\n",
        "        im = ax.imshow(reconstruct_pconn(test_data[i+2]), cmap='hot')\n",
        "        ax.get_xaxis().set_visible(False)\n",
        "        ax.get_yaxis().set_visible(False)\n",
        "        if i == 0:\n",
        "            ax.set_title('Subject 1')\n",
        "        if i == 1:\n",
        "            ax.set_title('Original Parcellated Connectivity Matrix\\nSubject 2')\n",
        "        if i ==2:\n",
        "            ax.set_title('Subject 3')\n",
        "\n",
        "        # plot generated pconn\n",
        "        ax = plt.subplot(2, n, i+1+n)\n",
        "        im = ax.imshow(reconstruct_pconn(decoded_pconns[i+2]), cmap='hot')\n",
        "        ax.get_xaxis().set_visible(False)\n",
        "        ax.get_yaxis().set_visible(False)\n",
        "        if i == 0:\n",
        "            ax.set_title('Subject 1')\n",
        "        if i == 1:\n",
        "            ax.set_title('Reconstructed Parecellated Connectivity Matrix\\nSubject 2')\n",
        "        if i == 2:\n",
        "            ax.set_title('Subject 3')\n",
        "    fig.subplots_adjust(bottom=0.1, top=0.9, left=0.1, right=0.8, wspace=0.02, hspace=0.02)\n",
        "    cb_ax = fig.add_axes([0.83,0.1,0.02,0.8])\n",
        "    cbar = fig.colorbar(im, cax=cb_ax)\n",
        "    #fig.colorbar(im)\n",
        "    plt.savefig('results.png')\n",
        "\n",
        "#def plot_autoencoder_outputs(autoencoder, test_data):\n",
        "#    decoded_pconns = autoencoder.predict(test_data)\n",
        "#    fig, axs = plt.subplots(nrows=2, ncols=2)\n",
        "#    im1 = axs[0,0].imshow(reconstruct_pconn(test_data[1]), cmap='hot')\n",
        "#    im2 = axs[0,1].imshow(reconstruct_pconn(test_data[3]), cmap='hot') \n",
        "#    im3 = axs[1,0].imshow(reconstruct_pconn(decoded_pconns[1]), cmap='hot')\n",
        "#    im4 = axs[1,1].imshow(reconstruct_pconn(decoded_pconns[3]), cmap='hot')\n",
        "#    fig.colorbar()\n",
        "        \n",
        "\n",
        "def reconstruct_pconn(pconn_vec):\n",
        "    pconn_vec=normalize(pconn_vec)\n",
        "    iu = np.triu_indices(NUM_PARCELS, 1)\n",
        "    recon_pconn = np.zeros((NUM_PARCELS, NUM_PARCELS))\n",
        "    np.fill_diagonal(recon_pconn,1)\n",
        "    recon_pconn[iu] = pconn_vec\n",
        "    recon_pconn = recon_pconn + recon_pconn.T - np.diag(np.diag(recon_pconn))\n",
        "    return(recon_pconn)\n",
        "\n",
        "def normalize(vec):\n",
        "    return 2*(vec-min(vec))/(max(vec)-min(vec))-1\n",
        "    \n",
        "\n",
        "def load_pconn_data(data_dir):\n",
        "    # Load all pconns in the data directory into a numpy array from a txt\n",
        "    # All pconns are created from the Gordon parcellation containing 352 parcels (size: 352x352)\n",
        "    # Get just the lower triangle of the matrix\n",
        "    \n",
        "    pconn_list = os.listdir(data_dir)\n",
        "\n",
        "    # Initialize empty numpy array\n",
        "    pconn_data = np.empty((len(pconn_list), 61776))\n",
        "\n",
        "    # indices of the upper triangle\n",
        "    iu = np.triu_indices(NUM_PARCELS, 1)\n",
        "    for i, pconn in enumerate(pconn_list):\n",
        "        pconn_path = os.path.join(data_dir, pconn)\n",
        "        pconn_mat = np.loadtxt(pconn_path)\n",
        "        pconn_mat = np.clip(pconn_mat, -1, 1)\n",
        "        pconn_tri = pconn_mat[iu]\n",
        "        pconn_data[i] = pconn_tri\n",
        "\n",
        "    test_data = pconn_data[0:int(len(pconn_list) * SPLIT)-1]\n",
        "    train_data = pconn_data[int(len(pconn_list) * SPLIT):-1]\n",
        "\n",
        "    print(train_data.shape)\n",
        "    print(test_data.shape)\n",
        "    return (train_data, test_data)\n",
        "\n",
        "def plot_training(training):\n",
        "\n",
        "    # Plot training & validation loss values\n",
        "    plt.plot(training.history['loss'])\n",
        "    plt.plot(training.history['val_loss'])\n",
        "    plt.title('Autoencoder Mean Squared Error')\n",
        "    plt.ylabel('MSE Loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.legend(['Train', 'Test'], loc='upper left')\n",
        "    plt.show() \n",
        "\n",
        "def load_autoencoder(h5_path):\n",
        "    autoencoder = load_model(return_path, compile=False) \n",
        "\n",
        "def build_autoencoder():\n",
        "    input_size = 61776\n",
        "    hidden_size = 432\n",
        "    hidden2_size = 48\n",
        "    latent_size = 12\n",
        "\n",
        "    # Build encoder\n",
        "\n",
        "    input_pconn = Input(shape=(input_size,))\n",
        "    d1 = Dense(hidden_size, activation='relu', kernel_regularizer=WeightsOrthogonalityConstraint(hidden_size, weightage=1., axis=0), activity_regularizer=UncorrelatedFeaturesConstraint(hidden_size, weightage = 1.), kernel_constraint=UnitNorm(axis=0))\n",
        "    d2 = Dense(hidden2_size, activation='relu', kernel_regularizer=WeightsOrthogonalityConstraint(hidden2_size, weightage=1., axis=0), activity_regularizer=UncorrelatedFeaturesConstraint(hidden2_size, weightage = 1.), kernel_constraint=UnitNorm(axis=0))\n",
        "    d3 = Dense(latent_size, activation='relu', kernel_regularizer=WeightsOrthogonalityConstraint(latent_size, weightage=1., axis=0), activity_regularizer=UncorrelatedFeaturesConstraint(latent_size, weightage = 1.), kernel_constraint=UnitNorm(axis=0))\n",
        "    hidden_1 = d1(input_pconn)\n",
        "    hidden2_1 = d2(hidden_1)\n",
        "    latent = d3(hidden2_1)\n",
        "    \n",
        "    encoder = Model(input_pconn, latent, name='encoder')\n",
        "    encoder.summary()\n",
        "\n",
        "    # Build decoder\n",
        "\n",
        "    latent_inputs = Input(shape=(latent_size,), name='decoder_input')\n",
        "    #hidden2_2 = Dense(hidden2_size, activation='relu')(latent_inputs)\n",
        "    #hidden_2 = Dense(hidden_size, activation='relu')(hidden2_2)\n",
        "    #output_pconn = Dense(input_size, activation='sigmoid')(hidden_2)\n",
        "    td3 = DenseTied(hidden2_size, activation='relu', kernel_constraint=UnitNorm(axis=1), tied_to=d3)\n",
        "    td2 = DenseTied(hidden_size, activation='relu', kernel_constraint=UnitNorm(axis=1), tied_to=d2)\n",
        "    td1 = DenseTied(input_size, activation='sigmoid', kernel_constraint=UnitNorm(axis=1), tied_to=d1)\n",
        "    hidden2_2 = td3(latent_inputs)\n",
        "    hidden_2 = td2(hidden2_2)\n",
        "    output_pconn = td1(hidden_2)\n",
        "\n",
        "    decoder = Model(latent_inputs, output_pconn, name=\"decoder\")\n",
        "    decoder.summary()\n",
        "    \n",
        "    # Build autoencoder = encoder + decoder\n",
        "    #autoencoder = Model(input_pconn, output_pconn)\n",
        "    autoencoder = Model(input_pconn, decoder(encoder(input_pconn)), name='autoencoder')\n",
        "    autoencoder.summary()\n",
        "    opt = Adam(lr=0.001)\n",
        "    autoencoder.compile(optimizer=opt, loss='mean_squared_error')\n",
        "    \n",
        "    return (autoencoder, encoder, decoder)\n",
        "\n",
        "\n",
        "def train_autoencoder(train_data, test_data):\n",
        "\n",
        "    (autoencoder, encoder, decoder) = build_autoencoder()        \n",
        "\n",
        "    \n",
        "    #encoder = Model(inputs=autoencoder.input, outputs=autoencoder.get_layer('encoder').output)\n",
        "\n",
        "    training = autoencoder.fit(train_data, train_data, validation_split=SPLIT, epochs=20, batch_size=1, verbose=1)\n",
        "\n",
        "    plot_training(training)\n",
        "\n",
        "\n",
        "    return (autoencoder, encoder, decoder)\n",
        "\n",
        "\n",
        "def main():\n",
        "\n",
        "    data_dir = '/home/exacloud/lustre1/fnl_lab/projects/VAE/data/test_pconn_data'\n",
        "\n",
        "    (train_data, test_data) = load_pconn_data(data_dir)\n",
        "\n",
        "    autoencoder, encoder, decoder = train_autoencoder(train_data, test_data)\n",
        "    autoencoder.save('gordon_pconn_autoencoder.h5')\n",
        "    encoder.save('gordon_pconn_encoder.h5')\n",
        "    encoder.save_weights('gordon_pconn_encoder_weights.h5')\n",
        "    decoder.save('gordon_pconn_decoder.h5')\n",
        "    decoder.save_weights('gordon_pconn_decoder_weights.h5')\n",
        "    \n",
        "    #autoencoder = load_model('/home/exacloud/lustre1/fnl_lab/projects/VAE/code/gordon_pconn_autoencoder.h5', compile=False) \n",
        "     \n",
        "    plot_autoencoder_outputs(autoencoder, test_data)\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "2cLDKxw2xQu5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}